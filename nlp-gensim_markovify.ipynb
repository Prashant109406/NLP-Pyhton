{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exercise 1. Text Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: markovify in c:\\users\\prashant mourya\\anaconda3\\lib\\site-packages (0.9.0)\n",
      "Requirement already satisfied: unidecode in c:\\users\\prashant mourya\\anaconda3\\lib\\site-packages (from markovify) (1.2.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install markovify"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import markovify #Markov Chain Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>publish_date</th>\n",
       "      <th>headline_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>20030219</td>\n",
       "      <td>aba decides against community broadcasting lic...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>20030219</td>\n",
       "      <td>act fire witnesses must be aware of defamation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>20030219</td>\n",
       "      <td>a g calls for infrastructure protection summit</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   publish_date                                      headline_text\n",
       "0      20030219  aba decides against community broadcasting lic...\n",
       "1      20030219     act fire witnesses must be aware of defamation\n",
       "2      20030219     a g calls for infrastructure protection summit"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inp = pd.read_csv('abcnews-date-text.csv')\n",
    "inp.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_model = markovify.NewlineText(inp.headline_text, state_size = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wilkie keeps independents at sa zoo welcomes new australians take fuel plea for information\n",
      "live earth message heard around the world of sport judo camp\n",
      "australian to win test\n",
      "accused triple murderer sentenced to 17 years of btn filming in sydney international\n",
      "all silent victorious in spain\n",
      "qld police dont know how important are chinas non performing loans reaching a tipping point\n",
      "fairfax shares slump on recession fears\n",
      "crime falls in full swing on iraq thursday\n",
      "earthquake strikes near flashpoint city of perth\n",
      "man charged over istanbul bombings\n"
     ]
    }
   ],
   "source": [
    "# Print ten randomly-generated sentences using the built model\n",
    "for i in range(10):\n",
    "    print(text_model.make_sentence())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exercise 2. Text Summarization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gensim in c:\\users\\prashant mourya\\anaconda3\\lib\\site-packages (3.8.3)\n",
      "Requirement already satisfied: Cython==0.29.14 in c:\\users\\prashant mourya\\anaconda3\\lib\\site-packages (from gensim) (0.29.14)\n",
      "Requirement already satisfied: smart-open>=1.8.1 in c:\\users\\prashant mourya\\anaconda3\\lib\\site-packages (from gensim) (3.0.0)\n",
      "Requirement already satisfied: scipy>=0.18.1 in c:\\users\\prashant mourya\\anaconda3\\lib\\site-packages (from gensim) (1.3.1)\n",
      "Requirement already satisfied: six>=1.5.0 in c:\\users\\prashant mourya\\anaconda3\\lib\\site-packages (from gensim) (1.12.0)\n",
      "Requirement already satisfied: numpy>=1.11.3 in c:\\users\\prashant mourya\\anaconda3\\lib\\site-packages (from gensim) (1.16.5)\n",
      "Requirement already satisfied: requests in c:\\users\\prashant mourya\\anaconda3\\lib\\site-packages (from smart-open>=1.8.1->gensim) (2.22.0)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in c:\\users\\prashant mourya\\anaconda3\\lib\\site-packages (from requests->smart-open>=1.8.1->gensim) (3.0.4)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in c:\\users\\prashant mourya\\anaconda3\\lib\\site-packages (from requests->smart-open>=1.8.1->gensim) (1.24.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\prashant mourya\\anaconda3\\lib\\site-packages (from requests->smart-open>=1.8.1->gensim) (2019.9.11)\n",
      "Requirement already satisfied: idna<2.9,>=2.5 in c:\\users\\prashant mourya\\anaconda3\\lib\\site-packages (from requests->smart-open>=1.8.1->gensim) (2.8)\n"
     ]
    }
   ],
   "source": [
    "!pip install gensim\n",
    "\n",
    "from gensim.summarization import summarize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = open(\"jumper.txt\", \"r\").read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary:\n",
      "Now married with four children, Wilkinson was told she would need spinal surgery not just to dive, but to safely do everyday activities with her children.\n",
      "Wilkinson had the surgery and rushed to get back into the water ahead of the 2020 Olympic trials.\n"
     ]
    }
   ],
   "source": [
    "print ('Summary:')\n",
    "print (summarize(text))\n",
    "clean_data=summarize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keywords:\n",
      "wilkinson\n",
      "olympic\n",
      "spinal\n",
      "diving\n",
      "dive\n",
      "major world\n",
      "titles\n",
      "revealed\n",
      "foot\n",
      "drop\n",
      "flipped\n",
      "laura\n",
      "told\n"
     ]
    }
   ],
   "source": [
    "from gensim.summarization import keywords\n",
    "\n",
    "print ('Keywords:')\n",
    "print (keywords(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.summarization import mz_keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Prashant Mourya\\Anaconda3\\lib\\site-packages\\gensim\\summarization\\mz_entropy.py:65: RuntimeWarning: divide by zero encountered in log2\n",
      "  log_p = np.log2(p)\n",
      "C:\\Users\\Prashant Mourya\\Anaconda3\\lib\\site-packages\\gensim\\summarization\\mz_entropy.py:66: RuntimeWarning: invalid value encountered in multiply\n",
      "  h = np.nan_to_num(p * log_p).sum(axis=0)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('strong', 0.972271386430644),\n",
       " ('in', 0.24954917386951747),\n",
       " ('her', 5.6982134305716414e-05)]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mz_keywords(text,scores=True,weighted=False,blocksize=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exercise 3. Topic Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import NMF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = open('clean_data.txt','w')\n",
    "file.write(clean_data)\n",
    "file.close()\n",
    "data = open('clean_data.txt','r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 1)\t0.26714212365374107\n",
      "  (0, 5)\t0.26714212365374107\n",
      "  (0, 11)\t0.26714212365374107\n",
      "  (0, 4)\t0.26714212365374107\n",
      "  (0, 6)\t0.26714212365374107\n",
      "  (0, 13)\t0.1900738211945578\n",
      "  (0, 12)\t0.26714212365374107\n",
      "  (0, 8)\t0.26714212365374107\n",
      "  (0, 14)\t0.26714212365374107\n",
      "  (0, 17)\t0.1900738211945578\n",
      "  (0, 3)\t0.5342842473074821\n",
      "  (0, 7)\t0.26714212365374107\n",
      "  (1, 15)\t0.3776277807406418\n",
      "  (1, 9)\t0.3776277807406418\n",
      "  (1, 0)\t0.3776277807406418\n",
      "  (1, 2)\t0.3776277807406418\n",
      "  (1, 16)\t0.3776277807406418\n",
      "  (1, 10)\t0.3776277807406418\n",
      "  (1, 13)\t0.26868527618515564\n",
      "  (1, 17)\t0.26868527618515564\n",
      "X =  ['2020' 'activities' 'ahead' 'children' 'dive' 'everyday' 'just' 'married'\n",
      " 'need' 'olympic' 'rushed' 'safely' 'spinal' 'surgery' 'told' 'trials'\n",
      " 'water' 'wilkinson']\n"
     ]
    }
   ],
   "source": [
    "# converting the given text term-document matrix\n",
    " \n",
    "vectorizer = TfidfVectorizer(max_features=2000, min_df=1, stop_words='english')\n",
    "X = vectorizer.fit_transform(data)\n",
    "idx_to_word = np.array(vectorizer.get_feature_names())\n",
    "\n",
    "print(X)\n",
    "print(\"X = \", idx_to_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 1: olympic,trials,ahead,rushed,2020,water,everyday,just,married,safely,activities,told,dive,spinal,surgery,children,wilkinson,need\n",
      "Topic 2: 2020,trials,ahead,rushed,olympic,water,just,surgery,dive,wilkinson,safely,spinal,activities,married,need,children,everyday,told\n",
      "Topic 3: need,activities,children,dive,everyday,just,married,told,spinal,safely,ahead,2020,rushed,wilkinson,surgery,trials,water,olympic\n",
      "Topic 4: need,activities,told,children,dive,everyday,just,married,spinal,safely,wilkinson,surgery,olympic,2020,trials,water,ahead,rushed\n",
      "Topic 5: 2020,trials,ahead,rushed,olympic,water,need,wilkinson,married,safely,dive,activities,everyday,spinal,surgery,told,just,children\n",
      "Topic 6: dive,told,safely,married,spinal,need,just,everyday,activities,children,water,wilkinson,2020,rushed,ahead,trials,surgery,olympic\n",
      "Topic 7: 2020,trials,ahead,rushed,olympic,water,need,married,wilkinson,surgery,safely,spinal,children,activities,everyday,told,just,dive\n",
      "Topic 8: spinal,need,married,dive,safely,just,activities,told,children,everyday,2020,ahead,olympic,trials,rushed,surgery,water,wilkinson\n",
      "Topic 9: olympic,trials,ahead,rushed,2020,water,told,everyday,married,spinal,safely,dive,activities,surgery,wilkinson,just,need,children\n",
      "Topic 10: olympic,trials,ahead,rushed,2020,water,children,told,spinal,everyday,need,activities,dive,surgery,just,married,safely,wilkinson\n",
      "Topic 11: olympic,trials,ahead,rushed,2020,water,everyday,wilkinson,need,married,spinal,activities,safely,told,dive,surgery,just,children\n",
      "Topic 12: 2020,trials,ahead,rushed,olympic,water,wilkinson,dive,surgery,told,just,need,safely,everyday,married,spinal,activities,children\n",
      "Topic 13: married,need,spinal,safely,dive,just,told,children,activities,everyday,2020,water,ahead,rushed,trials,olympic,surgery,wilkinson\n",
      "Topic 14: need,activities,told,children,dive,everyday,just,married,spinal,safely,2020,trials,wilkinson,surgery,water,rushed,olympic,ahead\n",
      "Topic 15: need,activities,told,children,dive,everyday,just,married,spinal,safely,surgery,olympic,wilkinson,water,ahead,rushed,trials,2020\n",
      "Topic 16: married,need,spinal,safely,dive,just,told,activities,children,everyday,wilkinson,trials,olympic,ahead,2020,surgery,rushed,water\n",
      "Topic 17: activities,olympic,trials,safely,rushed,water,ahead,just,2020,everyday,married,need,dive,children,spinal,told,surgery,wilkinson\n",
      "Topic 18: 2020,trials,ahead,rushed,olympic,water,wilkinson,married,surgery,just,activities,everyday,safely,told,dive,need,spinal,children\n",
      "Topic 19: 2020,ahead,trials,rushed,water,olympic,told,surgery,activities,spinal,wilkinson,everyday,just,dive,married,safely,need,children\n",
      "Topic 20: olympic,trials,ahead,rushed,2020,water,dive,surgery,just,everyday,activities,wilkinson,told,need,spinal,safely,married,children\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Prashant Mourya\\Anaconda3\\lib\\site-packages\\sklearn\\decomposition\\nmf.py:1069: ConvergenceWarning: Maximum number of iteration 200 reached. Increase it to improve convergence.\n",
      "  \" improve convergence.\" % max_iter, ConvergenceWarning)\n"
     ]
    }
   ],
   "source": [
    "nmf = NMF(n_components=20, solver=\"mu\")\n",
    "W = nmf.fit_transform(X)\n",
    "H = nmf.components_\n",
    "\n",
    "for i, topic in enumerate(H):\n",
    "     print(\"Topic {}: {}\".format(i + 1, \",\".join([str(x) for x in idx_to_word[topic.argsort()[-20:]]])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
